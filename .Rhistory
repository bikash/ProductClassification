### Simulation & Modeling of Hidden Markov Model (HMM)
library(TeachingDemos)
library(HMM)
library(ggplot2)
set.seed(1)
### Define our variables
TPM <- matrix(c(.95, .05,
.1, .9), 2, byrow = TRUE)
EPM <- matrix(c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6,
1/10, 1/10, 1/10, 1/10, 1/10, 1/2), 2, byrow = TRUE)
simulations <- 500
### Create a dataframe to hold our results
dice <- rep(NA, simulations)
number <- rep.int(0, simulations)
results <- data.frame(dice, number)
### Simulate
# Assume we start with a fair dice
state <- "FAIR"
for (i in 1:simulations) {
if (state == "FAIR") {
# Check to see if we're staying with a FAIR dice
p <- runif(1)
if (p <= TPM[1,2]) {
# If not, roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember new state
state <- "LOADED"
}
else {
# Roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember old state
state <- "FAIR"
}
}
if (state == "LOADED") {
# Check to see if we're staying with a LOADED dice
p <- runif(1)
if (p < TPM[2,1]) {
# If not, roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember new state
state <- "FAIR"
}
else {
# Roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember old state
state <- "LOADED"
}
}
# Save dice roll and state
results[i, 1] <- state
results[i, 2] <- roll
}
install.packages("TeachingDemos")
### Simulation & Modeling of Hidden Markov Model (HMM)
library(TeachingDemos)
library(HMM)
library(ggplot2)
set.seed(1)
### Define our variables
TPM <- matrix(c(.95, .05,
.1, .9), 2, byrow = TRUE)
EPM <- matrix(c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6,
1/10, 1/10, 1/10, 1/10, 1/10, 1/2), 2, byrow = TRUE)
simulations <- 500
### Create a dataframe to hold our results
dice <- rep(NA, simulations)
number <- rep.int(0, simulations)
results <- data.frame(dice, number)
### Simulate
# Assume we start with a fair dice
state <- "FAIR"
for (i in 1:simulations) {
if (state == "FAIR") {
# Check to see if we're staying with a FAIR dice
p <- runif(1)
if (p <= TPM[1,2]) {
# If not, roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember new state
state <- "LOADED"
}
else {
# Roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember old state
state <- "FAIR"
}
}
if (state == "LOADED") {
# Check to see if we're staying with a LOADED dice
p <- runif(1)
if (p < TPM[2,1]) {
# If not, roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember new state
state <- "FAIR"
}
else {
# Roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember old state
state <- "LOADED"
}
}
# Save dice roll and state
results[i, 1] <- state
results[i, 2] <- roll
}
### Simulation & Modeling of Hidden Markov Model (HMM)
library(TeachingDemos)
library(HMM)
library(ggplot2)
set.seed(1)
### Define our variables
TPM <- matrix(c(.95, .05,
.1, .9), 2, byrow = TRUE)
EPM <- matrix(c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6,
1/10, 1/10, 1/10, 1/10, 1/10, 1/2), 2, byrow = TRUE)
simulations <- 500
### Create a dataframe to hold our results
dice <- rep(NA, simulations)
number <- rep.int(0, simulations)
results <- data.frame(dice, number)
### Simulate
# Assume we start with a fair dice
state <- "FAIR"
for (i in 1:simulations) {
if (state == "FAIR") {
# Check to see if we're staying with a FAIR dice
p <- runif(1)
if (p <= TPM[1,2]) {
# If not, roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember new state
state <- "LOADED"
}
else {
# Roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember old state
state <- "FAIR"
}
}
if (state == "LOADED") {
# Check to see if we're staying with a LOADED dice
p <- runif(1)
if (p < TPM[2,1]) {
# If not, roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember new state
state <- "FAIR"
}
else {
# Roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember old state
state <- "LOADED"
}
}
# Save dice roll and state
results[i, 1] <- state
results[i, 2] <- roll
}
### Modeling
# Create hmm using our TPM/EPM
hmm <- initHMM(c("FAIR", "LOADED"), c(1, 2, 3, 4, 5, 6),
transProbs = TPM, emissionProbs = EPM)
# Pull in results from the simulation
obs <- results[, 2]
# Save Viterbi/Posterior predictions as a new column
results$viterbi <- viterbi(hmm, obs)
results$posterior <- posterior(hmm, obs)[1, ]
results$posterior[results$posterior >= 0.5] <- "FAIR"
results$posterior[results$posterior < 0.5] <- "LOADED"
# Check out results
table(results$dice)
table(results$viterbi)
table(results$posterior)
### Plot predictions with true sequence
p1 <- ggplot(aes(x = seq_along(dice)), data = results) +
geom_point(aes(y = dice)) +
ylab("State") + xlab("Dice Roll (In Sequence)") + ylab("State") +
ggtitle("Actual Results")
p2 <- ggplot(aes(x = seq_along(dice)), data = results) +
geom_point(aes(y = dice), color = "#F8766D") +
geom_point(aes(y = viterbi), color = "#00BFC4") +
xlab("Dice Roll (In Sequence)") + ylab("State") +
ggtitle("Viterbi Predictions")
p3 <- ggplot(aes(x = seq_along(dice)), data = results) +
geom_point(aes(y = dice), color = "#F8766D") +
geom_point(aes(y = posterior), color = "#00BFC4") +
xlab("Dice Roll (in sequence)") + ylab("State") +
ggtitle("Posterior Predictions")
grid.arrange(p1, p2, p3, ncol = 1)
### Plot predictions with true sequence
p1 <- ggplot(aes(x = seq_along(dice)), data = results) +
geom_point(aes(y = dice)) +
ylab("State") + xlab("Dice Roll (In Sequence)") + ylab("State") +
ggtitle("Actual Results")
### setting path of repo folder.
setwd("/Users/bikash/repos/kaggle/facebook-recuriting/")
#setwd("/home/ekstern/haisen/bikash/kaggle/RestaurantRevenuePrediction/")
library(party)
library(e1071)
library(lubridate)
library(Boruta)
library(gtools)
#load data
train = read.csv("data/train.csv", header = TRUE, stringsAsFactors = FALSE)
test = read.csv("data/test.csv", header = TRUE, stringsAsFactors = FALSE)
bids = read.csv("data/bids.csv", header = TRUE, stringsAsFactors = FALSE)
df_train = merge( train, bids, by='bidder_id', all=FALSE, sort= T)
df_test = merge( test, bids, by='bidder_id', all=FALSE, sort= T)
head(df_train)
as.factor(df_train$device)
summary(df_train$device)
head(df_train)
as.factor(df_train$country)
head(df_train)
as.factor(df_train$merchandise)
head(df_train)
as.factor(df_train$auction)
summary(df_train$auction)
summary(df_train)
head(df_train)
df_train$is.Robot <- ifelse(round(df_train$outcome,0)==1,1,0)
df_train$is.Robot
df_train[1387]
df_train[[1387]]
df_train[1387,]
df_train[1388,]
df_train[1389,]
df_train[1392,]
df_train[df_train$is.Robot == 1]
df_train[df_train$outcome == 1]
df_train[df_train$outcome == 1,]
head(df_train[df_train$outcome == 1,] )
tail(df_train[df_train$outcome == 1,] )
length(df_train$auction)
length(df_test$auction)
df_train[df_train$outcome == 1,]
#################################################################################################
#################################################################################################
## Author: Bikash Agrawal
## Date: 12th March Feb 2015
## Email: er.bikash21@gmail.com
## Description: Random Forest to classify product in Otto Group Kaggle Product classification competition.
##     http://www.kaggle.com/c/otto-group-product-classification-challenge
##
## References:
## [1] http://dnene.bitbucket.org/docs/mlclass-notes/lecture16.html
## [2]
##
#################################################################################################
#################################################################################################
### setting path of repo folder.
getwd()
setwd("/Users/bikash/repos/kaggle/ProductClassification/")
devtools::install_github('dmlc/xgboost',subdir='R-package')
library(xgboost)
require(methods)
##########################################################################
########Cleaning up training dataset #####################################
##########################################################################
print("Data Cleaning up process......")
train <- read.csv("data/train.csv", header=TRUE)
test <- read.csv("data/test.csv", header=TRUE)
train = train[,-1]
test = test[,-1]
set.seed(1234)
y = train[,ncol(train)]
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-ncol(train)],test)
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
head(train)
x
head9x
head(x)
trind
train <- read.csv("data/train.csv", header=TRUE)
test <- read.csv("data/test.csv", header=TRUE)
features <- train[, c(-1, -95)]
library(ggplot2)
library(readr)
library(Rtsne)
tsne <- Rtsne(as.matrix(features), check_duplicates = FALSE, pca = TRUE, perplexity = 30, theta = 0.5, dims = 2)
embedding <- as.data.frame(tsne$Y)
embedding$Class <- as.factor(sub("Class_", "", train[,95]))
pic <- ggplot(embedding, aes(x=V1, y=V2, color=Class)) + geom_point(size=1.25)
pic <- pic + guides(colour = guide_legend(override.aes = list(size=6)))
pic <- pic + xlab("") + ylab("") + ggtitle("t-SNE 2D Embedding of Products Data")
pic <- pic + theme_light(base_size=20) + theme(strip.background = element_blank(),
strip.text.x     = element_blank(),
axis.text.x      = element_blank(),
axis.text.y      = element_blank(),
axis.ticks       = element_blank(),
axis.line        = element_blank(),
panel.border     = element_blank())
pic
ggsave("graph/tsne.png", pic, width=8, height=6, units="in")
head(features)
head(embedding)
head(features)
res <- prcomp(train[, 2:94], center = TRUE, scale = FALSE)
names(res)
head(features)
head(res)
head(train[, 2:94])
explain90 <- min(which(cumsum(res$sdev^2/sum(res$sdev^2)) > 0.9))
print(cumsum(res$sdev^2/sum(res$sdev^2))[explain90])
pic.pca <- ggplot( ) + geom_point(aes(x = 1:93, y = cumsum(res$sdev^2/sum(res$sdev^2))), colour = "#5DA5DA", size = 3)
pic.pca
pic.pca <- pic.pca + geom_point(aes(x = explain90, y = cumsum(res$sdev^2/sum(res$sdev^2))[explain90]), colour = "#F15854", size = 3)
pic.pca
pic.pca <- pic.pca + annotate("text", x = 43, y = 0.93, label = paste(as.character(round(cumsum(res$sdev^2/sum(res$sdev^2))[explain90], digits = 4) * 100), "%", sep = ""))
pic.pca <- pic.pca + annotate("text", x = 43, y = 0.88, label = "PC43")
pic.pca <- pic.pca + ggtitle("Principal Component Analysis") + xlab("Principal Component") + ylab("Variance Explained")
pic.pca
trunc <- res$x[,1 : explain90] %*% t(res$rotation[,1 : explain90])
if(res$scale != FALSE){
trunc <- scale(trunc, center = FALSE , scale=1/res$scale)
}
if(res$center != FALSE){
trunc <- scale(trunc, center = -1 * res$center, scale=FALSE)
}
trunc
head(trunc)
train.pca <- train
train.pca[, 2 : 94] <- trunc
head(train.pca)
pic1 <- ggplot(data = NULL, aes(x = as.character(unique(train$target)), y = as.numeric(table(train$target)))) + geom_bar(stat="identity", fill = "#5DA5DA")
pic1 <- pic1 + ggtitle("Class Distribution") + xlab("Classes") + ylab("Count")
pic1 <- pic1 + geom_text(aes(label = as.numeric(table(train$target))), vjust = -0.2)
pic
ggsave("graph/dist.png", pic, width=8, height=6, units="in")
ggsave("graph/pca.png", pic.pca, width=8, height=6, units="in")
