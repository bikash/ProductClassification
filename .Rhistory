### Simulation & Modeling of Hidden Markov Model (HMM)
library(TeachingDemos)
library(HMM)
library(ggplot2)
set.seed(1)
### Define our variables
TPM <- matrix(c(.95, .05,
.1, .9), 2, byrow = TRUE)
EPM <- matrix(c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6,
1/10, 1/10, 1/10, 1/10, 1/10, 1/2), 2, byrow = TRUE)
simulations <- 500
### Create a dataframe to hold our results
dice <- rep(NA, simulations)
number <- rep.int(0, simulations)
results <- data.frame(dice, number)
### Simulate
# Assume we start with a fair dice
state <- "FAIR"
for (i in 1:simulations) {
if (state == "FAIR") {
# Check to see if we're staying with a FAIR dice
p <- runif(1)
if (p <= TPM[1,2]) {
# If not, roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember new state
state <- "LOADED"
}
else {
# Roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember old state
state <- "FAIR"
}
}
if (state == "LOADED") {
# Check to see if we're staying with a LOADED dice
p <- runif(1)
if (p < TPM[2,1]) {
# If not, roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember new state
state <- "FAIR"
}
else {
# Roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember old state
state <- "LOADED"
}
}
# Save dice roll and state
results[i, 1] <- state
results[i, 2] <- roll
}
install.packages("TeachingDemos")
### Simulation & Modeling of Hidden Markov Model (HMM)
library(TeachingDemos)
library(HMM)
library(ggplot2)
set.seed(1)
### Define our variables
TPM <- matrix(c(.95, .05,
.1, .9), 2, byrow = TRUE)
EPM <- matrix(c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6,
1/10, 1/10, 1/10, 1/10, 1/10, 1/2), 2, byrow = TRUE)
simulations <- 500
### Create a dataframe to hold our results
dice <- rep(NA, simulations)
number <- rep.int(0, simulations)
results <- data.frame(dice, number)
### Simulate
# Assume we start with a fair dice
state <- "FAIR"
for (i in 1:simulations) {
if (state == "FAIR") {
# Check to see if we're staying with a FAIR dice
p <- runif(1)
if (p <= TPM[1,2]) {
# If not, roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember new state
state <- "LOADED"
}
else {
# Roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember old state
state <- "FAIR"
}
}
if (state == "LOADED") {
# Check to see if we're staying with a LOADED dice
p <- runif(1)
if (p < TPM[2,1]) {
# If not, roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember new state
state <- "FAIR"
}
else {
# Roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember old state
state <- "LOADED"
}
}
# Save dice roll and state
results[i, 1] <- state
results[i, 2] <- roll
}
### Simulation & Modeling of Hidden Markov Model (HMM)
library(TeachingDemos)
library(HMM)
library(ggplot2)
set.seed(1)
### Define our variables
TPM <- matrix(c(.95, .05,
.1, .9), 2, byrow = TRUE)
EPM <- matrix(c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6,
1/10, 1/10, 1/10, 1/10, 1/10, 1/2), 2, byrow = TRUE)
simulations <- 500
### Create a dataframe to hold our results
dice <- rep(NA, simulations)
number <- rep.int(0, simulations)
results <- data.frame(dice, number)
### Simulate
# Assume we start with a fair dice
state <- "FAIR"
for (i in 1:simulations) {
if (state == "FAIR") {
# Check to see if we're staying with a FAIR dice
p <- runif(1)
if (p <= TPM[1,2]) {
# If not, roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember new state
state <- "LOADED"
}
else {
# Roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember old state
state <- "FAIR"
}
}
if (state == "LOADED") {
# Check to see if we're staying with a LOADED dice
p <- runif(1)
if (p < TPM[2,1]) {
# If not, roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember new state
state <- "FAIR"
}
else {
# Roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember old state
state <- "LOADED"
}
}
# Save dice roll and state
results[i, 1] <- state
results[i, 2] <- roll
}
### Modeling
# Create hmm using our TPM/EPM
hmm <- initHMM(c("FAIR", "LOADED"), c(1, 2, 3, 4, 5, 6),
transProbs = TPM, emissionProbs = EPM)
# Pull in results from the simulation
obs <- results[, 2]
# Save Viterbi/Posterior predictions as a new column
results$viterbi <- viterbi(hmm, obs)
results$posterior <- posterior(hmm, obs)[1, ]
results$posterior[results$posterior >= 0.5] <- "FAIR"
results$posterior[results$posterior < 0.5] <- "LOADED"
# Check out results
table(results$dice)
table(results$viterbi)
table(results$posterior)
### Plot predictions with true sequence
p1 <- ggplot(aes(x = seq_along(dice)), data = results) +
geom_point(aes(y = dice)) +
ylab("State") + xlab("Dice Roll (In Sequence)") + ylab("State") +
ggtitle("Actual Results")
p2 <- ggplot(aes(x = seq_along(dice)), data = results) +
geom_point(aes(y = dice), color = "#F8766D") +
geom_point(aes(y = viterbi), color = "#00BFC4") +
xlab("Dice Roll (In Sequence)") + ylab("State") +
ggtitle("Viterbi Predictions")
p3 <- ggplot(aes(x = seq_along(dice)), data = results) +
geom_point(aes(y = dice), color = "#F8766D") +
geom_point(aes(y = posterior), color = "#00BFC4") +
xlab("Dice Roll (in sequence)") + ylab("State") +
ggtitle("Posterior Predictions")
grid.arrange(p1, p2, p3, ncol = 1)
### Plot predictions with true sequence
p1 <- ggplot(aes(x = seq_along(dice)), data = results) +
geom_point(aes(y = dice)) +
ylab("State") + xlab("Dice Roll (In Sequence)") + ylab("State") +
ggtitle("Actual Results")
install.packages("Rtsne")
### setting path of repo folder.
getwd()
setwd("/Users/bikash/repos/kaggle/ProductClassification/")
library(Rtsne)
train <- read.csv("data/train.csv", stringsAsFactors=FALSE)[, -1]
set.seed(1234)
tsne_out_train <- Rtsne(as.matrix(train[,1:93]), check_duplicates = FALSE, pca = TRUE,
max_iter = 1000, perplexity=30, theta=0.5, dims=2, verbose=TRUE)
my_palette = c("red", "blue", "green", "brown", "magenta", "orange", "cyan", "black", "yellow")
palette(my_palette)
tsne_out_train <- Rtsne(as.matrix(train[,1:93]), check_duplicates = FALSE, pca = TRUE,
max_iter = 1000, perplexity=30, theta=0.5, dims=2, verbose=TRUE)
tsne_out_train <- Rtsne(as.matrix(train[,1:93]), check_duplicates = FALSE, pca = TRUE,
max_iter = 1000, perplexity=30, theta=0.5, dims=2)
tsne_out_train <- Rtsne(as.matrix(train[,1:93]), check_duplicates = FALSE, pca = TRUE,
perplexity=30, theta=0.5, dims=2)
my_palette = c("red", "blue", "green", "brown", "magenta", "orange", "cyan", "black", "yellow")
palette(my_palette)
plot(tsne_out_train$Y, xlab="", ylab="", col=as.factor(train$target), pch=".", cex=4, axes=FALSE)
legend("bottomleft", c("1","2", "3", "4", "5", "6", "7", "8", "9"),
lty=c(1,1), lwd=c(5,5), col=my_palette, bty="n", cex = 0.7)
install.packages("~/Downloads/h2o-2.8.4.4/R/h2o_2.8.4.4.tar.gz",
repos = NULL, type = "source")
install.packages("h2o")
library(h2o)
h2o.init()
### setting path of repo folder.
getwd()
setwd("/Users/bikash/repos/kaggle/ProductClassification/")
library(dplyr)
library(zoo)
library(randomForest)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(lattice)
library(Amelia) ## Amelia is packages to display missing data using missmap function
library(mclust)
sessionInfo()
##########################################################################
########Cleaning up training dataset #####################################
##########################################################################
print("Data Cleaning up process......")
train <- read.csv("data/train.csv", header=TRUE)
test <- read.csv("data/test.csv", header=TRUE)
#################################################
train1 = train[1:5000,]
test1 = train[5001:10000,]
train1 = train[1:5000,]
test1 = train[5001:10000,]
train2 <- train1[,-1]
set.seed(12)
fit <- randomForest(as.factor(target) ~ ., data=train2, ntree=500, method = "class")
library(randomForest)
install.packages("randomForest")
library(randomForest)
set.seed(12)
fit <- randomForest(as.factor(target) ~ ., data=train2, ntree=500, method = "class")
train1 = train[1:10000,]
test1 = train[10001:15000,]
train2 <- train1[,-1]
set.seed(12)
fit <- randomForest(as.factor(target) ~ ., data=train2, ntree=500, method = "class")
train1 = train[1:30000,]
test1 = train[30001:40000,]
train2 <- train1[,-1]
set.seed(12)
fit <- randomForest(as.factor(target) ~ ., data=train2, ntree=500, method = "class")
head(train) ## column name -> id, feat_1,.......feat_93, target
train1 = train[1:50000,]
test1 = train[50001:60000,]
test1 = train[50001:60000,]
train2 <- train1[,-1]
fit <- randomForest(as.factor(target) ~ ., data=train2, ntree=500, method = "class")
tail(train)
train1 = train[1:55000,]
test1 = train[55001:60000,]
train2 <- train1[,-1]
fit <- randomForest(as.factor(target) ~ ., data=train2, ntree=500, method = "class")
train1 = train[1:60000,]
test1 = train[60001:67000,]
train2 <- train1[,-1]
fit <- randomForest(as.factor(target) ~ ., data=train2, ntree=500, method = "class")
View(train)
library(ada)
install.packages("ada")
library(ada)
##########################################################################
########Cleaning up training dataset #####################################
##########################################################################
print("Data Cleaning up process......")
train <- read.csv("data/train.csv", header=TRUE)
test <- read.csv("data/test.csv", header=TRUE)
##########################################################################
train1 = train[1:10000,]
test1 = train[10001:67000,]
default=rpart.control()
gdis<-ada(y~.,data=train1,iter=50,loss="e",type="discrete",control=default)
gdis<-ada(target~.,data=train1,iter=50,loss="e",type="discrete",control=default)
plot(gdis)
pairs(gdis,train[,-1],maxvar=3)
install.packages("gbm")
install.packages("gbm")
library(gm)
library(gbm)
gbm(target~., data = train1, distribution = "bernoulli", interaction.depth = 6, shrinkage = 0.07, n.trees = 800, verbose = TRUE,
n.cores = 3, train.fraction = 1, bag.fraction = 0.5,
n.minobsinnode = 10)
gbm(target~., data = train1,  n.trees = 800,
n.cores = 3, train.fraction = 1, bag.fraction = 0.5,
n.minobsinnode = 10)
m <-gbm(target~., data = train1,  n.trees = 800)
m <-glm(target~., data = train1,  n.trees = 800)
m <-glm(target~., data = train1)
m
devtools::install_github('dmlc/xgboost',subdir='R-package')
require(xgboost)
require(methods)
install.packages(xgboost)
install.packages("xgboost")
devtools::install_github('tqchen/xgboost',subdir='R-package')
install.packages('xgboost')
library(xgboost)
require(methods)
devtools::install_github('dmlc/xgboost',subdir='R-package')
library(xgboost)
require(methods)
devtools::install_github('dmlc/xgboost',subdir='R-package')
devtools::install_github('tqchen/xgboost',subdir='R-package')
install.packages("FNN")
library(FNN)
##########################################################################
print("Data Cleaning up process......")
train <- read.csv("data/train.csv", header=TRUE)
test <- read.csv("data/test.csv", header=TRUE)
##########################################################################
# make target a factor
train$target = as.factor(train$target)
head(train)
# remove target
train <- train[,-95]
# remove ID cols
train <- train[,-1]
test <- test[,-1]
head(train)
results <- data.frame(knn(train, test, classes, k = 9, prob=TRUE))
names(results) <- c("Label")
results$ImageId <- 1:nrow(results)
results <- results[c(2,1)]
write.csv(results, file = "output/knn_submit.csv", quote = FALSE, row.names = FALSE)
classes <- train[,95]
classes
train <- read.csv("data/train.csv", header=TRUE)
test <- read.csv("data/test.csv", header=TRUE)
train$target = as.factor(train$target)
#
classes <- train[,95]
# remove target
train <- train[,-95]
classes
train <- train[,-1]
test <- test[,-1]
results <- data.frame(knn(train, test, classes, k = 9, prob=TRUE))
results
names(results) <- c("Label")
results$ImageId <- 1:nrow(results)
results <- results[c(2,1)]
results
head(results)
write.csv(results, file = "output/knn_submit.csv", quote = FALSE, row.names = FALSE)
head(results)
#Load dependency
library(nnet)
# set seed
set.seed(1337)
### setting path of repo folder.
getwd()
setwd("/Users/bikash/repos/kaggle/ProductClassification/")
#load data
##########################################################################
########Cleaning up training dataset #####################################
##########################################################################
print("Data Cleaning up process......")
train <- read.csv("data/train.csv", header=TRUE)
test <- read.csv("data/test.csv", header=TRUE)
##########################################################################
# fit and predict
fit<-nnet(target ~ ., train[,-1], size = 3, rang = 0.1, decay = 5e-4, maxit = 500)
predicted<-as.data.frame(predict(fit,test[,-1],type="raw"))
